## Extending Partially Recorded Samples Using CAC and SVS
---

Music is one of the widely used unstructured data especially by consumer for entertainment. This is because music can share messages and emotions that we can relate to. Some music consists of many instruments played together at the same time while some is a piece played by a solo instrument. Human voice is also one of the instruments that we like to use and sing music.

Music is severely unstructured because of the difficulty in extracting interesting information from the music file and their ability to fit in different kinds of information at once. Examples of high-level feature that can be extracted from music files are the mood of the song, the genre of the song, etc. These high-level features can be summarised based on the low-level features in a song such as notes, chords, etc, which can also be equally as hard to extract.

A singing voice is a monophonic instrument that can only produce one musical note at the time. This makes it slightly easier, given a frame of time, to estimate the fundamental frequencies, which is the melody that is sung in the singing voice. Using a machine learning approach, can we teach computer to generate the best completion from the input singing voice?

The main goal of this project is to build a computer-assisted composition (CAC) model to assist the completion of the input singing voice while synthesising the output voice having the similar quality (timbre) with the input singing voice. To synthesise the singing voice, a Singing Voice Synthesis (SVS) program can be used such as Vocaloid, UTAU, etc. In this research, the open source UTAU program will become the project's SVS program. The combination of note generation and voice synthesis makes the project harder to be completed in a short duration of time. Hence, the open source SVS program can help to reduce the focus on the voice synthesis to supplying the note track and selection of voice bank to sing.

To achieve the goal, the proposed framework begins with the training of Sequence-to-Sequence pitch and time model as the components of the CAC model. The input training data consists of the vocal track of the midi files from popular pop English musics. While training, the generation of singing audio begins concurrently with the extraction of note and its corresponding duration and the MFCC features of the partially recorded singing voice. The note and note duration features will be passed to the CAC model to generate new note track, and the MFCC will be used as a main factor for the selection of sound bank for the SVS to sing. Finally, the expected output will continue the music from the input voice. The following graph illustrate the flow of the training of the CAC model and the generation of singing voice using SVS and CAC.

![Project Framework](images/projects/cac-svs/theoreticalSVSarchitecturaldiagram.png)

The implementation of the projects is still under progress. Stay tuned for the part two of the outcome of the project.
